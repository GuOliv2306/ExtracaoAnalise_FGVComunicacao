{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA4748Snp2RI"
      },
      "source": [
        "# Modelagem de Tópicos — LDA (scikit-learn)\n",
        "\n",
        "> Prof. Matheus C. Pestana (FGV Comunicação Rio)\n",
        "\n",
        "#### Caminho básico\n",
        "\n",
        "1) Pré-processamento leve\n",
        "\n",
        "2) Vetorização (Bag-of-Words)\n",
        "\n",
        "3) LDA (Latent Dirichlet Allocation) em scikit-learn\n",
        "\n",
        "4) BERTopic (com embeddings) para comparação - outro notebook\n",
        "\n",
        "##### Observações didáticas:\n",
        "- LDA trabalha sobre a matriz termo-documento (contagens), buscando \"misturas\" de tópicos.\n",
        "- BERTopic combina embeddings + clustering + c-TF-IDF para temas mais coerentes em muitos casos.\n",
        "- Rodar BERTopic pode levar mais tempo na 1ª vez (download do modelo de embeddings).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8zPi1cdpr6m"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgz91S4OqMmt"
      },
      "source": [
        "Vamos carregar uma base chamada 20newsgroups, de listas de emails temáticas. É possível usar qualquer outra base do pandas para fazer a alteração."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwV_4PTJqLxt",
        "outputId": "a427501b-12da-4297-86d6-3ad532e91f54"
      },
      "outputs": [],
      "source": [
        "print(\"[INFO] Carregando 20 Newsgroups (apenas 'train')...\")\n",
        "raw = fetch_20newsgroups(subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"))\n",
        "docs = raw.data\n",
        "print(f\"[INFO] Total de documentos disponíveis: {len(docs)}\")\n",
        "\n",
        "# Vamos limitar a 2000 para fins de exemplificação\n",
        "max_n = 2000\n",
        "if len(docs) > max_n:\n",
        "    random.seed(42)\n",
        "    idx = random.sample(range(len(docs)), max_n)\n",
        "    docs = [docs[i] for i in idx]\n",
        "print(f\"[INFO] Usando {len(docs)} documentos para a aula.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssi6OJ54qa-z"
      },
      "source": [
        "## 1) Limpeza do texto (obrigatória para LDA)\n",
        "\n",
        "Vamos limpar o texto, removendo múltiplos espaços em branco, mantendo letras e pontuação básica. Poderíamos remover stopwords também, mas isso acarretaria em \"overclean\". O próprio vetorizador cuidará disso..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbIQ6laFqY9b",
        "outputId": "877a7bb2-86ed-4bf8-c083-deb4937bae26"
      },
      "outputs": [],
      "source": [
        "def simple_clean(s):\n",
        "    s = s or \"\" # Transforma em texto o que não for\n",
        "    s = re.sub(r\"\\s+\", \" \", s)  # espaços múltiplos em branco -> 1\n",
        "    s = s.strip() # Remove os espaços do início e do final\n",
        "    return s\n",
        "\n",
        "docs = [simple_clean(d) for d in docs]\n",
        "print(\"[INFO] Limpeza leve concluída.\")\n",
        "\n",
        "# Qual a outra forma de limpar os documentos?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrNQMceIqz9v"
      },
      "source": [
        "## 2) Vetorização\n",
        "\n",
        "A vetorização é o processo de transformar textos — que são dados não estruturados — em uma representação numérica que os algoritmos conseguem entender. Em vez de lidar com frases ou parágrafos diretamente, o computador cria uma matriz de termos por documentos: cada coluna representa uma palavra (ou combinação de palavras) e cada linha representa um documento. Assim, a presença, a frequência ou o peso desses termos são registrados em números que podem ser usados em modelos como LDA ou BERTopic.\n",
        "\n",
        "Nesse processo, usamos algumas técnicas importantes:\n",
        "\n",
        "•\tStopwords: são palavras muito frequentes, mas que carregam pouca informação semântica (como “the”, “of”, “em”, “de”). Retirá-las ajuda a reduzir ruído e destacar termos realmente relevantes.\n",
        "\n",
        "•\tN-gramas: enquanto unigramas consideram apenas palavras isoladas, bigrams e trigrams capturam combinações de 2 ou 3 palavras consecutivas (ex.: “inteligência artificial”, “ciência de dados”), o que enriquece a interpretação dos tópicos.\n",
        "\n",
        "•\tmin_df: define a frequência mínima para que um termo seja mantido. Palavras que aparecem em pouquíssimos documentos são descartadas por não contribuírem de forma consistente.\n",
        "\n",
        "•\tmax_df: faz o oposto, eliminando palavras que aparecem em quase todos os documentos, pois tendem a não diferenciar os textos.\n",
        "\n",
        "Em resumo, vetorização é o passo em que transformamos linguagem em números, equilibrando o que manter e o que descartar para que os modelos consigam identificar padrões significativos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqkvGNBAsLiG",
        "outputId": "782d04ed-f8d1-476b-959c-7ab741cf322b"
      },
      "outputs": [],
      "source": [
        "# Como seria o mesmo conjunto de textos (mais simples) vetorizado em ambas?\n",
        "textos = [\n",
        "          'oi, meu nome é joão',\n",
        "          'oi, tudo bem?',\n",
        "          '\"joão e maria\" é uma história para crianças',\n",
        "          'nossa história não acabará'\n",
        "          ]\n",
        "\n",
        "count = CountVectorizer()\n",
        "X_count = count.fit_transform(textos)\n",
        "print(pd.DataFrame(X_count.toarray(), columns=count.get_feature_names_out()))\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(textos)\n",
        "print(pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4gJw06lsYES"
      },
      "source": [
        "### CountVectorizer\n",
        "\n",
        "O CountVectorizer é a forma mais simples de vetorização de textos: ele constrói uma matriz em que cada coluna corresponde a um termo (palavra ou n-grama) e cada célula contém a contagem absoluta de vezes que aquele termo aparece em um documento. Por exemplo, se a palavra “dados” aparece 3 vezes em um texto, a célula correspondente recebe o valor 3. Isso gera uma matriz esparsa (muitos zeros), que funciona bem para modelos como LDA, pois esse método parte da ideia de que os documentos são misturas de tópicos e tópicos são misturas de palavras frequentes.\n",
        "\n",
        "### TfidfVectorizer\n",
        "\n",
        "O TfidfVectorizer (Term Frequency–Inverse Document Frequency) também cria uma matriz termo-documento, mas em vez de apenas contar frequências, ele pesa cada termo de acordo com sua importância relativa.\n",
        "\n",
        "•\tO TF (Term Frequency) mede quantas vezes a palavra aparece em um documento.\n",
        "•\tO IDF (Inverse Document Frequency) mede o quão rara a palavra é no conjunto de documentos: termos muito comuns (como “dia”, “ano”, “gente”) recebem peso baixo, enquanto termos mais raros e distintivos recebem peso alto.\n",
        "\n",
        "Assim, o TF-IDF dá um peso maior a termos que ajudam a diferenciar documentos, e não apenas a termos muito repetidos. Essa abordagem costuma ser mais eficaz em tarefas como classificação, busca de informação e clustering com embeddings, pois favorece termos discriminativos em vez de palavras genéricas.\n",
        "\n",
        "#### Comparação resumida\n",
        "\n",
        "•\tCountVectorizer: simples, usa contagem bruta de palavras\n",
        "\n",
        "•\tTfidfVectorizer: usa ponderação TF-IDF (frequência + raridade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysxla_-Eqy0B",
        "outputId": "4857319f-b7f1-44c2-cb74-9c27e6ea9e21"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1, 2),\n",
        "    max_df=0.95,\n",
        "    min_df=5\n",
        ")\n",
        "X_count = vectorizer.fit_transform(docs)\n",
        "terms_count = np.array(vectorizer.get_feature_names_out())\n",
        "print(f\"[INFO] Matriz termo-documento: {X_count.shape} (docs x termos)\")\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1, 2),\n",
        "    max_df=0.95,\n",
        "    min_df=5\n",
        ")\n",
        "X_tfidf = vectorizer.fit_transform(docs)\n",
        "terms_tfdf = np.array(vectorizer.get_feature_names_out())\n",
        "print(f\"[INFO] Matriz termo-documento: {X_tfidf.shape} (docs x termos)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js0YEnhDr1Mk",
        "outputId": "1be6e328-79d5-4b93-fc22-bf49ce693090"
      },
      "outputs": [],
      "source": [
        "print(f\"Com CountVectorizer:\")\n",
        "print(pd.DataFrame(X_count.toarray(), columns=terms_count).head())\n",
        "print(f\"Com TfidfVectorizer:\")\n",
        "print(pd.DataFrame(X_tfidf.toarray(), columns=terms_tfdf).head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUmooTBDuGTn"
      },
      "source": [
        "## 3) LDA - Alocação Latente de Dirichlet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3vZGk7ktabI",
        "outputId": "d97cf0eb-10ca-458b-adf3-af0e6b5b792a"
      },
      "outputs": [],
      "source": [
        "n_topics = 8\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_topics,\n",
        "    learning_method=\"batch\",\n",
        "    random_state=42\n",
        ")\n",
        "print(\"[INFO] Ajustando LDA...\")\n",
        "W = lda.fit_transform(X_tfidf)  # distribuição doc->tópicos\n",
        "H = lda.components_             # distribuição tópico->termos (contagens \"suavizadas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhpxfzR6ujyI",
        "outputId": "7b1d83aa-25de-492a-d49e-f39ee96bc082"
      },
      "outputs": [],
      "source": [
        "def show_top_words(H, terms, topn=12):\n",
        "    for k, row in enumerate(H):\n",
        "        top_idx = row.argsort()[::-1][:topn]\n",
        "        top_terms = terms[top_idx]\n",
        "        print(f\"\\n[Tópico {k}]\")\n",
        "        print(\", \".join(top_terms))\n",
        "\n",
        "print(\"\\n======== LDA: Top palavras por tópico ========\")\n",
        "show_top_words(H, terms_tfdf, topn=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXCiHxTRvSkN"
      },
      "source": [
        "### Como achar o melhor número de tópicos?\n",
        "\n",
        "#### ELBO (Evidence Lower Bound)\n",
        "\n",
        "O ELBO, ou Evidence Lower Bound, é uma medida interna usada quando aplicamos inferência variacional para ajustar modelos probabilísticos, como o LDA.\n",
        "\n",
        "•\tEm teoria, gostaríamos de calcular a verossimilhança (likelihood) completa dos dados dado o modelo, mas isso é inviável porque exige integrais muito complexas.\n",
        "\n",
        "•\tEm vez disso, usamos uma aproximação variacional: escolhemos uma distribuição aproximada para os parâmetros e otimizamos essa aproximação.\n",
        "\n",
        "•\tO ELBO é justamente o limite inferior da verossimilhança — quanto maior o ELBO, melhor a aproximação variacional está representando a probabilidade verdadeira dos dados.\n",
        "\n",
        "Resumindo... o ELBO serve como critério de otimização e diagnóstico interno. Modelos com maior ELBO estão explicando melhor os dados dentro do espaço de aproximação escolhido.\n",
        "\n",
        "#### Perplexidade\n",
        "\n",
        "A perplexidade é uma métrica mais interpretável, derivada da log-verossimilhança, que mede o quão “surpreso” o modelo fica ao ver um novo conjunto de dados.\n",
        "\n",
        "•\tFormalmente, é a exponencial da entropia média dos documentos no modelo.\n",
        "\n",
        "•\tIntuitivamente, valores baixos de perplexidade indicam que o modelo prevê bem as distribuições de palavras nos documentos, ou seja, que ele está menos “perplexo” com o que aparece.\n",
        "\n",
        "Resumindo... quanto menor a perplexidade, melhor o modelo está capturando a estrutura dos dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7xO-L44unfn",
        "outputId": "d9d7bddb-48e8-4b95-e254-3d93c9454bd2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Dividir em treino/teste para avaliar generalização\n",
        "X_train, X_test = train_test_split(X_tfidf, test_size=0.2, random_state=42)\n",
        "\n",
        "k_candidates = [4, 5, 6, 7, 8, 10, 12, 15, 20]  # ajuste livremente\n",
        "results = []\n",
        "\n",
        "print(\"\\n[INFO] Avaliando diferentes k para LDA...\")\n",
        "for k in k_candidates:\n",
        "    lda_k = LatentDirichletAllocation(\n",
        "        n_components=k,\n",
        "        learning_method=\"batch\",\n",
        "        random_state=42\n",
        "    )\n",
        "    lda_k.fit(X_train)\n",
        "\n",
        "    train_ll = lda_k.score(X_train)         # log-likelihood (ELBO), maior melhor\n",
        "    test_perp = lda_k.perplexity(X_test)    # perplexidade no teste, menor melhor\n",
        "\n",
        "    results.append({\"k\": k, \"train_loglik\": train_ll, \"test_perplexity\": test_perp})\n",
        "    print(f\"  k={k:>2} | train_loglik={train_ll:>12.2f} | test_perplexity={test_perp:>10.2f}\")\n",
        "\n",
        "# Organizar resultados\n",
        "res_df = pd.DataFrame(results).sort_values([\"test_perplexity\", \"train_loglik\"], ascending=[True, False])\n",
        "print(\"\\n[INFO] Resultados ordenados (melhor perplexidade primeiro):\")\n",
        "print(res_df)\n",
        "\n",
        "# Sugerir k (menor perplexidade)\n",
        "best_k = res_df.iloc[0][\"k\"]\n",
        "print(f\"\\n[SUGESTÃO] Melhor k pela menor perplexidade de teste: k={int(best_k)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.plot([r[\"k\"] for r in results], [r[\"test_perplexity\"] for r in results], marker=\"o\")\n",
        "plt.xlabel(\"k (número de tópicos)\")\n",
        "plt.ylabel(\"Perplexidade (teste) — menor é melhor\")\n",
        "plt.title(\"Seleção de k para LDA (perplexidade)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNgbMABuwhda"
      },
      "source": [
        "# Qual o problema dessas métricas?\n",
        "\n",
        "•\tO ELBO é usado durante o treinamento (critério de ajuste interno).\n",
        "\n",
        "•\tA perplexidade é usada como métrica de avaliação (quanto o modelo generaliza bem para novos documentos).\n",
        "\n",
        "•\tContudo, ambas não garantem interpretabilidade: às vezes um modelo com perplexidade melhor pode gerar tópicos pouco coerentes. Por isso, sempre se complementa com análise qualitativa ou métricas de coerência de tópicos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERTopic\n",
        "\n",
        "https://colab.research.google.com/drive/1JyYNCQel1YaePZ_ARNz1CQT55ijYi5YQ#scrollTo=qOgead0wzCRR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URUHBQ5mvdaY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
